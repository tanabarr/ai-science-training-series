{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5228d",
   "metadata": {},
   "source": [
    "# Fitting Data with a Line\n",
    "\n",
    "Many of you have probably used Excel to fit data with a line.\n",
    "\n",
    "![excel_linear_regression](img/excel_linear_regression.jpg)\n",
    "\n",
    "We're going to review the math involved in this process to help understand how training an AI works.\n",
    "\n",
    "First we will load some tools that others wrote and we can use to help us work.\n",
    "* [Pandas](https://pandas.pydata.org/docs/): a toolkit for working with row vs. column data, like excel sheets, and CSV (Comma Seperated Values) files.\n",
    "* [Numpy](https://numpy.org/doc/): a toolkit for managing arrays, vectors, matrices, etc, doing math with them, slicing them up, and many other handy things.\n",
    "* [Matplotlib](https://matplotlib.org/stable/index.html): a toolkit for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_machineLearning/slimmed_realestate_data.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipydis\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e287b5",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "In the [previous notebook](00_make_slimmed_dataset.ipynb) we used a realestate dataset from Kaggle to produce this reduced dataset. This dataset contains the _sale price_ and _above ground square feet_ of many houses. We can use this data for our linear regression.\n",
    "\n",
    "We use Pandas to read the data file which is stored as Comma Separated Values (CSV). and print the column labels. CSV files are similar to excel sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('slimmed_realestate_data.csv')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cef2a4",
   "metadata": {},
   "source": [
    "Now pandas provides some helpful tools for us to inspect our data. It provides a `plot()` function that, behind the scenes, is calling into the _Matplotlib_ library and calling the function [matplotlib.pyplot.plot()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html). In this case, we simply tell it the names of the columns we want as our _x_ and _y_ values and the `style` (`'.'` tells `matplotlib` to use a small dot to represent each data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x='GrLivArea',y='SalePrice',style='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccc11b",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "If you have data, $(x,y)$, that you think can be fit by a line, $y = m x + b$, then there are [simple equations](https://en.wikipedia.org/wiki/Simple_linear_regression) one can use to calculate the slope ($m$) and intercept ($b$).\n",
    "\n",
    "They are:\n",
    "\n",
    "$$m = { n (\\Sigma xy) - (\\Sigma x) (\\Sigma y) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } $$\n",
    "\n",
    "$$b = { (\\Sigma y) (\\Sigma x^2) - (\\Sigma x) (\\Sigma xy) \\over n (\\Sigma x^2) - (\\Sigma x)^2 } $$\n",
    "\n",
    "We'll break this calculation into a few steps to help make it easier.\n",
    "\n",
    "First lets define $x$ and $y$. $x$ will be our _above ground square footage_ and $y$ will be _sale price_. In our equations we have a few different values we need, such as $n$ which is just the number of points we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d486286",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96578d6",
   "metadata": {},
   "source": [
    "Then we need our $x$ and $y$ by selecting only the column we care about for each one. Note about data formats: `data` is a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) object which has rows and columns; `data['GrLivArea']` is a Pandas [Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object which only has rows; then we also convert from _Pandas_ data formats (in this case a _Series_) to _Numpy_ data formats using the `to_numpy()` function which is part of the Pandas _Series_ object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2465ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['GrLivArea'].to_numpy()\n",
    "y = data['SalePrice'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62181d",
   "metadata": {},
   "source": [
    "Now we will calculate $\\Sigma xy$, $\\Sigma x$, $\\Sigma y$, and $\\Sigma x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10636a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_xy = np.sum(x*y)\n",
    "sum_x = np.sum(x)\n",
    "sum_y = np.sum(y)\n",
    "sum_x2 = np.sum(x*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c6cb6",
   "metadata": {},
   "source": [
    "The denominator in the equation for $m$ and $b$ are the same so we can calculate that once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b826a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = n * sum_x2 - sum_x * sum_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b90ef",
   "metadata": {},
   "source": [
    "Then we can calculate our fit values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = (n * sum_xy - sum_x * sum_y) / denominator\n",
    "b = (sum_y * sum_x2 - sum_x * sum_xy) / denominator\n",
    "print('y = %f * x + %f' % (m,b))\n",
    "\n",
    "# saving these for later comparison\n",
    "m_calc = m\n",
    "b_calc = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178aa7a",
   "metadata": {},
   "source": [
    "Now we can plot the fit results with our data to see how we did. \n",
    "\n",
    "First we define a plotting function because we're going to do this often and we want to reuse our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cade97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x,y,m,b,plt = plt):\n",
    "   # plot our data points with 'bo' = blue circles\n",
    "   plt.plot(x,y,'bo')\n",
    "   # create the line based on our linear fit\n",
    "   # first we need to make x points\n",
    "   # the 'arange' function generates points between two limits (min,max)\n",
    "   linear_x = np.arange(x.min(),x.max())\n",
    "   # now we use our fit parameters to calculate the y points based on our x points\n",
    "   linear_y = linear_x * m + b\n",
    "   # plot the linear points using 'r-' = red line\n",
    "   plt.plot(linear_x,linear_y,'r-',label='fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55e58",
   "metadata": {},
   "source": [
    "Now can use this function to plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x,y,m,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcee22",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "SGD is a common method in AI for training deep neural networks on large datasets. It is an iterative method for optimizing a loss function that we get to define. We will use this simple linear regression to demonstrate how it works.\n",
    "\n",
    "# The model\n",
    "\n",
    "In AI, neural networks are often referred to as a _model_ because, once fully trained, they should model (AKA predict) the behavior of our system. In our example, the system is how house prices vary based on house size. We know our system is roughly driven by a linear function:\n",
    "\n",
    "$$ \\hat{y_i}(x_i) = m * x_i + b $$\n",
    "\n",
    "We just need to figure out $m$ and $b$. Let's create a function that calculates our model given $x$, $m$, and $b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edd196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,m,b):\n",
    "   return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5e8e7",
   "metadata": {},
   "source": [
    "\n",
    "# The Loss Function\n",
    "\n",
    "A _loss function_, or _objective function_, is something we define and is based on what we want to achieve. In the method of SGD, it is our goal to minimize (or make close to zero) the values calculated from the _loss function_. In our example, we ideally want the prediction of our _model_ to be equal to the actual data, though we will settle for \"as close as possible\".\n",
    "\n",
    "So we will select our _loss function_ to be the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) function:\n",
    "\n",
    "$$ L(y_i,\\hat{y_i}) = (y_i - \\hat{y_i}(x_i))^2 $$\n",
    "\n",
    "where $y_i$ is our $i^{th}$ entry in the `data['SalePrice']` vector and $\\hat{y_i}$ is the prediction based on evaluting $m * x_i + b$.\n",
    "\n",
    "This function looks like the figure below when we plot it with $x=y_i - \\hat{y_i}(x_i)$ and we we want to be down near $y_i - \\hat{y_i}(x_i) = 0$ which indicates that our $y_i$ is as close as possible to $\\hat{y_i}$.\n",
    "\n",
    "![loss_func](img/loss_func.png)\n",
    "\n",
    "\n",
    "Here we crate a function that calculates this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7676b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y,m,b):\n",
    "   y_predicted = model(x,m,b)\n",
    "   return np.power( y - y_predicted, 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e4a33",
   "metadata": {},
   "source": [
    "# Minimizing the Loss Function\n",
    "\n",
    "We want to use the loss function in order to guide how to update $m$ and $b$ to better model our system. In calculus we learn to minimize a function with respect to a variable you calculate the _partial derivative_ with respect to the variable you want to vary.\n",
    "\n",
    "$$ { \\partial L \\over \\partial m } = 0 $$\n",
    "\n",
    "The location of the solution to this is the minimum as shown in the figure above. We can write down the partial derivative of the loss function as:\n",
    "\n",
    "$$ { \\partial L \\over \\partial m } = -2 x_i (y_i - \\hat{y_i}(x_i)) $$\n",
    "$$ { \\partial L \\over \\partial b } = -2 (y_i - \\hat{y_i}(x_i)) $$\n",
    "\n",
    "We can use this to calculate an adjustment to $m$ and $b$ that will reduce the loss function, effectively improving our fitting parameters. This is done using this equation:\n",
    "\n",
    "$$ m' = m - \\eta { \\partial L \\over \\partial m }$$\n",
    "$$ b' = b - \\eta { \\partial L \\over \\partial b }$$\n",
    "\n",
    "Here our original $m$ and $b$ are adjusted by the partial derivative multiplied by some small factor, $\\eta$, called the _learning rate_. This learning rate is very important in our process and must be tuned for every problem.\n",
    "\n",
    "In our example, the selection of the learning rate essentially defines how close we can get to the minimum, AKA the best fit solution. This figure shows what happens when we pick a large learning rate. We first select a starting point in our loss function (typically randomly), then every update from $m$/$b$ to $m'$/$b'$ results in a shift to somewhere else on our loss function (following the red arrows). In this example, our learning rate ($\\eta$) has been selected too large such that we bounce back and forth around the minimum, never reaching it.\n",
    "\n",
    "![largeLR](img/parabola_largeLR.png)\n",
    "\n",
    "If we select a smaller learning we can see better behavior in the next figure.\n",
    "\n",
    "![smallLR](img/parabola_smallLR.png)\n",
    "\n",
    "Though, keep in mind, too small a learning rate results is so little progress toward the minimum that you may never reach it!\n",
    "\n",
    "A pit fall of SGD that one must be aware of is when your loss function is complex, with many minima. The next figure shows such a case, in which we select a small learning rate and our starting point happens to be near a local minimum that is not the lowest minimum. As shown, we do reach a minimum, but it isn't the lowest minimum in our loss function. It could be that we randomly select a starting point near the minimum we care about, but we should build methods that are more robust against randomly getting the right answer.\n",
    "\n",
    "![local_min_smallLR](img/local_min_smallLR.png)\n",
    "\n",
    "Then, if we increase our learning rate too much, we bounce around again.\n",
    "\n",
    "![local_min_largeLR](img/local_min_largeLR.png)\n",
    "\n",
    "What we want to do in this situation is start with a large learning rate and slowly reduce its size as we progress. That is shown in this next figure.\n",
    "\n",
    "![local_min_variableLR](img/local_min_variableLR.png)\n",
    "\n",
    "As you can see, this process is not perfect and could still land in a local minimum, but it is important to be aware of these behaviors as you utilize SGD in machine learning.\n",
    "\n",
    "So let's continue, we'll build functions we can use to update our fit parameters, $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a75c95",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def updated_m(x,y,m,b,learning_rate):\n",
    "   dL_dm = - 2 * x * (y - model(x,m,b))\n",
    "   dL_dm = np.mean(dL_dm)\n",
    "   return m - learning_rate * dL_dm\n",
    "\n",
    "def updated_b(x,y,m,b,learning_rate):\n",
    "   dL_db = - 2 * (y - model(x,m,b))\n",
    "   dL_db = np.mean(dL_db)\n",
    "   return b - learning_rate * dL_db\n",
    "\n",
    "def apply_decay(epoch_num,initial_rate,decay_const):\n",
    "   dR = 1 + decay_const * epoch_num\n",
    "   return (1 / dR) * initial_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06004c",
   "metadata": {},
   "source": [
    "# Putting it together\n",
    "\n",
    "We can now randomly select our initial slope and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5.\n",
    "b = 1000.\n",
    "print('y_i = %.2f * x + %.2f' % (m,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58732dfc",
   "metadata": {},
   "source": [
    "Then we can calculate our Loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(x,y,m,b)\n",
    "print('first 10 loss values: ',l[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-9\n",
    "m = updated_m(x,y,m,b,learning_rate)\n",
    "b = updated_b(x,y,m,b,learning_rate)\n",
    "print('y_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f' % (m,b,m_calc,b_calc))\n",
    "plot_data(x,y,m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb7264",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# set our initial slope and intercept\n",
    "m = 5.\n",
    "b = 1000.\n",
    "batch_size = 60\n",
    "# set initial learning rate for each parameter\n",
    "initial_learning_rate_m = 1e-7\n",
    "initial_learning_rate_b = 1e-1\n",
    "# set decay rate tunable constant to be used when applying learning rate decay\n",
    "decay_rate = 1\n",
    "# use these to plot our progress over time\n",
    "loss_history = []\n",
    "\n",
    "# we run our loop N times\n",
    "loop_N = 30\n",
    "#loop_N = 3\n",
    "for i in range(loop_N):\n",
    "\n",
    "   # convert panda data to numpy arrays, one for the \"Ground Living Area\" and one for \"Sale Price\"\n",
    "   data_x = data['GrLivArea'].sample(n = batch_size).to_numpy()\n",
    "   data_y = data['SalePrice'].sample(n = batch_size).to_numpy()\n",
    "\n",
    "   #print('Processing %d random samples from GrLivArea Dataframe (data_x)' % len(data_x))\n",
    "   #print('Processing %d random samples from SalePrice Dataframe (data_y)' % len(data_y))\n",
    "\n",
    "   # apply learning rate decay for each parameter\n",
    "   learning_rate_m = apply_decay(i,initial_learning_rate_m,decay_rate)\n",
    "   learning_rate_b = apply_decay(i,initial_learning_rate_b,decay_rate)\n",
    "\n",
    "   # update our slope and intercept based on the current values\n",
    "   m = updated_m(data_x,data_y,m,b,learning_rate_m)\n",
    "   b = updated_b(data_x,data_y,m,b,learning_rate_b)\n",
    "\n",
    "   # calculate the loss value\n",
    "   loss_value = np.mean(loss(data_x,data_y,m,b))\n",
    "\n",
    "   # keep a history of our loss values\n",
    "   loss_history.append(loss_value)\n",
    "\n",
    "   # print our progress\n",
    "   print('[%03d]  dy_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f    loss: %f' % (i,m,b,m_calc,b_calc,loss_value))\n",
    "\n",
    "   # close/delete previous plots\n",
    "   plt.close('all')\n",
    "\n",
    "   # create a 1 by 2 plot grid\n",
    "   fig,ax = plt.subplots(1,2,figsize=(18,6),dpi=80)\n",
    "   # lot our usual output\n",
    "   plot_data(data_x,data_y,m,b,ax[0])\n",
    "\n",
    "   # here we also plot the calculated linear fit for comparison\n",
    "   line_x = np.arange(data_x.min(),data_x.max())\n",
    "   line_y = line_x * m_calc + b_calc\n",
    "   ax[0].plot(line_x,line_y,'b-',label='calculated')\n",
    "   # add a legend to the plot and x/y labels\n",
    "   ax[0].legend()\n",
    "   ax[0].set_xlabel('square footage')\n",
    "   ax[0].set_ylabel('sale price')\n",
    "\n",
    "   # plot the loss \n",
    "   loss_x = np.arange(0,len(loss_history))\n",
    "   loss_y = np.asarray(loss_history)\n",
    "   ax[1].plot(loss_x,loss_y)\n",
    "   ax[1].set_yscale('log')\n",
    "   ax[1].set_xlabel('loop step')\n",
    "   ax[1].set_ylabel('loss')\n",
    "   plt.show()\n",
    "   # gives us time to see the plot\n",
    "   time.sleep(2.5)\n",
    "   # clears the plot when the next plot is ready to show.\n",
    "   ipydis.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d4473",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b2744a",
   "metadata": {},
   "source": [
    "# In Class Exercises\n",
    "1. In AI, datasets are often very large and cannot be processed all at once as is done in the loop above. The data is instead randomly sampled in smaller _batches_ where each _batch_ contains `batch_size` inputs. How can you change the loop above to sample the dataset in smaller batches? Hint: Our `data` variable is a Pandas `DataFrame` object, search for \"how to sample a DataFrame\".\n",
    "2. As described above, learning rates that grow smaller over time can help find and get closer to global minima. In the loop above, our `learning_rate_m` and `learning_rate_b` are constant through the process of minimizing our parameters. How could you change the loop to reduce the learning rates over loop iterations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a44db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080d5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edd467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
